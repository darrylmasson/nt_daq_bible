\chapter{Introduction}\label{ch:intro}

If you're reading this, it probably means one of two things.
Either you're curious and want to learn about the XENONnT DAQ system, perhaps you wish to learn from our mistakes and do better, or (highly unlikely) you want to learn more about how the data you analyze is brought into existence in the first place.
Or, those who originally designed and built the system have left, it suddenly stopped working while the neutron generator was deployed, and Elena is yelling at you to make it work again, and yesterday.
If the second is true, you have my sympathy, but the best time to read this would have been last month.

\section{Design philosophy}\label{sec:philosophy}

To understand many of the decisions made in the design of the nT readout, we must consider things that did and didn't work in 1T.

\subsection{XENON1T}\label{sec:1tdaq}

The reader is assumed to have a detailed undestanding of the XENON1T daq paper.
I'm not going to go into great depth here.

The XENON1T DAQ was designed around the idea of a ``triggerless'' readout, in which the decision of trigger or not was delayed until the data was sitting around in memory of the readout servers, rather than in the memory of the digitizers.
XENON1T still had a trigger, it's just that this was a trigger implemented in software rather than in hardware.
While this was quite successful, it did suffer from a number of unforseeable errors.
Using a database as the central buffer is a good idea, until your rate of inserts begins to saturate the bandwidth your database can handle.
MongoDB suffers from this with its primary-secondary replication setup, because all writes must be directed to the primary, and it is but one node.
Also, you delete entries older than the trigger because you've already checked those entries and either decided ``save'' or ``don't save''.
MongoDB doesn't immediately free the resources a given document occupies when you delete it.
If you're trying to push a huge amount of data through this database, you want these resources to be made available as soon as possible.
There are workarounds, there always are, but workarounds tend to be less efficient.
As the XENONnT DAQ will be 3-4 times larger than that of 1T, the database would become more of a bottleneck.

The ``trigger'' was implemented in python (referred to semi-technically as the ``event-builder'', but not to be confused with the ``event-builder'' servers running the event-building software).
This software scanned the timestamps of inserts to the database for ``event-like'' patterns and then passed this information off to secondary machines for the actual rendering of events into a data format suitable for replication, storage, and processing.
The RabbitMQ package used for passing these events around between the servers was not ideal.
A minor memory leak somewhere in the chain caused it to crash about once per month.
Not major enough to get fixed, but still a minor annoyance for the duration of the experiment.
Also, it's still a trigger.
You couldn't go back and change the parameters after the fact because you've already made the save/delete decision.
By the end of the experiment this trigger had been deactivated and an actual ``triggerless'' system was running, but the entire architecture was still designed around the concept of a trigger.

Thus, the natural evolution of the system was to design around the idea of ``triggerless'', rather than merely adapt to it.

\subsection{XENONnT}

The changes made for nT were to enable the following requirements to be met:
\begin{itemize}
  \item Read out one PMT array twice
  \item Combined data stream with synchronized readout between different sub-detectors
  \item Triggerless readout compatible with strax
  \item Better uptime, stability, etc etc
\end{itemize}

\subsubsection{PMT readout}

The ADCs have a limited dynamic voltage range.
The V1724 accepts \SI{2.25}{\volt} or \SI{0.5}{\volt} (hardware modification), the V1730 accepts \SI{2}{\volt} or \SI{0.5}{\volt} (software-selection).
The single photon signals from a PMT are rather small, so they need to be amplified to reach the science goals of the experiment.
The typical amplification factor is 10, achieved via the Phillips 776.
Thus, if the signal output by the amplifier reaches \SI{2.25}{\volt}, the responsible digitizer channel saturates and the energy response becomes non-linear.
The PMT base can also saturate, but this happens at a \emph{much} higher energy (allegedly).
This non-linearity in the response significantly hampers the efforts of position reconstruction, which is a leading factor in the sensitivity to high-energy analyses like double-beta decay.
Thus, the solution was to read out the top PMT array once with amplification (for low-energy analyses) and once with attenuation (for high-energy purposes).
This means that rather than merely doubling in size with the number of PMTs, the readout triples with regards to 1T because half of the PMTs are read out twice.

\subsubsection{Inter-detector synchronization}

The synchronized starting and stopping of multiple (crates of) digitizers is handled most easily by the V2718 crate controller module, see~\ref{sec:v2718}.
That's not what I'm talking about here.
There was ``no'' synchronization between the TPC and MV in 1T.
They used the same \SI{50}{\mega\hertz} clock signal propagated from the last TPC board to the first MV board, but that's different (see~\ref{sec:digi_clocks}).
How do you ensure the start command issued by the dispatcher is registered by the software interfacing with the different detector CC's with the same $\mathcal{O}(\SI{10}{\nano\second})$ precision that each individual system has?
It is impossible unless the system is specifically designed to achieve this.
The ``synchronization'' in 1T was handled by digitizing the MV trigger signal with the AM.
This way, the analyist can see that something happened at ``this'' time in the MV, but can see nothing more.
You don't know what time this was in the MV, only what time it was in the TPC.
Granted, the MV rate is sufficiently low that it probably isn't ambiguous, but the point remains.

This was not sufficient for nT.
The analyst for nT should be able to see exactly what was happening in the veto subdetectors simultaneously with the TPC.
A better inter-detector integration was required.
This is achieved by fanning the start signal from the TPC CC to the other sub-detectors, and adding a considerable amount of additional logic into the dispatcher.
It is important that the signals are received by various digitizers within \SI{10}{\nano\second} of each other.
If, say, one detector was installed with an extension onto their end of the synchronized start cable, this ceases to be true.
This is a purely hypothetical example that in no way actually happened, no one is \emph{that} incompetent at their job, right?

\subsubsection{Strax compatibility}

Pax is another of those things that was a good idea at the time, but in hindsight certain issues were found.
Pax's memory model meant it was rather slow, and the insistance relatively early on that it support ROOT led to countless errors and issues that plagued analysts for the duration of the experiment (I know who you are, and I know how much you contributed to this support).
Pax's design was also grounded in the conventional idea of ``events'' and ``triggers''.
Strax was envisioned as a radically new way of thinking about data processing without the baggage of enforcing a predefined structure of events, and also with a more flexible paradigm of streaming data rather than monolithic ``runs''.
While the overall readout design is largely independent of the format you end up processing data in, a variety of optimizations can be made targetting specific rather than arbitrary storage/processing setups.
A full discussion of the strax architecture and design is beyond the scope of this document, but the basic gist is that the model centers around flat tables and fixed-width data.
The advantage of fixed-width is that it's very efficient for storage and processing for low-level reasons, so the work of converting the variable-width readout data into fixed-width format is the responsibility of the readout.

\subsubsection{Uptime}

For a system as fundamental and important as the PMT readout, it's important that it is ready for operation by the time the PMTs are installed, and that it is available more often than not.
The goings-on inside the detector are understood by means of the slow control and the fast readout.
The system shouldn't crash all the time, at least not once you've ironed out most of the main bugs.
Ideally you've already gotten many hundreds of terabytes of data from earlier in the design phase where most of the major issues were already found and fixed.
Growing pains are allowed, but it's a very bad look if the PMTs are installed and the readout isn't anything resembling ready.
Again, it's not like any of the vetos actually did this, that would be ridiculous, one of them was still installed and fully cabled and stuff.

\section{Sir Not-Appearing-In-This-Film}

This document is concerned turning PMT signals into disk usage.
Turning this disk usage into S1s, S2s, and events is touched on briefly but, as already stated, a full discussion of strax, its usage and nuances etc, is outside the scope of this document.
Also, making this data available to the analyist is Computing's problem, and will not be addressed here, although the Online Monitor is a DAQ-level piece.
